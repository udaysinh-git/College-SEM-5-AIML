================================================================================
LAB 1: LINEAR REGRESSION - SUPERVISED LEARNING
================================================================================
Keywords: Lab_1, Linear_Regression, Supervised_Learning
Dataset: Student Scores (Hours vs Marks)
--------------------------------------------------------------------------------

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
url = "http://bit.ly/w-data"
df = pd.read_csv(url)

# Visualize dataset info
print(df.head())

# Split into input (X) and target (Y)
X = df.iloc[:, :-1].values
Y = df.iloc[:, 1].values

# Split into training and testing sets (80-20)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize and train model
model = LinearRegression()
model.fit(X_train, Y_train)

# Predict test data
Y_pred = model.predict(X_test)

# Evaluate model
print('Mean Squared Error:', mean_squared_error(Y_test, Y_pred))
print('R2 Score:', r2_score(Y_test, Y_pred))

# Visualization
plt.scatter(X, Y, color='blue', label='Actual Data')
plt.plot(X, model.predict(X), color='red', label='Regression Line')
plt.xlabel('Hours Studied')
plt.ylabel('Scores')
plt.title('Linear Regression - Student Marks Prediction')
plt.legend()
plt.show()


================================================================================
LAB 1 (BONUS): CLASSIFICATION - LOGISTIC REGRESSION
================================================================================
Keywords: Lab_1, Classification, Logistic_Regression, Supervised_Learning
Dataset: Iris Dataset
--------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target
print(iris.data[:5])

# Split dataset (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression model
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# Predict test data
y_pred = model.predict(X_test)

# Evaluate model
print("Accuracy:", accuracy_score(y_test, y_pred))

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, cmap="Blues",
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Iris Classification - Logistic Regression")
plt.show()


================================================================================
LAB 2: K-MEANS CLUSTERING - UNSUPERVISED LEARNING
================================================================================
Keywords: Lab_2, K-Means_Clustering, Unsupervised_Learning
Dataset: Wine Dataset
--------------------------------------------------------------------------------

from sklearn.datasets import load_wine
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
wine = load_wine()
X = wine.data
df = pd.DataFrame(X, columns=wine.feature_names)

# K-Means with k=12
k = 12
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
labels = kmeans.fit_predict(X)

# Print cluster centers (short preview)
print("Cluster Centers (first 5 features):")
print(pd.DataFrame(kmeans.cluster_centers_[:, :5],
                   columns=wine.feature_names[:5]).round(3))

# Silhouette score
print("Silhouette Score:", silhouette_score(X, labels))

# Plot (alcohol vs malic acid)
plt.scatter(df["alcohol"], df["malic_acid"], c=labels, cmap="tab20")
plt.xlabel("Alcohol")
plt.ylabel("Malic Acid")
plt.title(f"K-Means Clustering (k={k})")
plt.show()


================================================================================
LAB 2 (PART 2): PCA - DIMENSIONALITY REDUCTION
================================================================================
Keywords: Lab_2, PCA, Dimensionality_Reduction, Unsupervised_Learning
Dataset: Wine Dataset
--------------------------------------------------------------------------------

from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
wine = load_wine()
X = wine.data
y = wine.target

# Scale data
X_scaled = StandardScaler().fit_transform(X)

# PCA with 4 components
n_components = 4
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# Print explained variance ratio
print("Explained Variance Ratio:", pca.explained_variance_ratio_)
print("Total Explained Variance:", sum(pca.explained_variance_ratio_))

# Plot PC1 vs PC2
plt.figure(figsize=(6,4))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y, palette="Set2")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA (PC1 vs PC2) - Wine Dataset")
plt.show()


================================================================================
LAB 2 (BONUS): PCA IMAGE COMPRESSION - MNIST
================================================================================
Keywords: Lab_2, PCA_Image_Compression, MNIST
Dataset: MNIST
--------------------------------------------------------------------------------

from tensorflow.keras.datasets import mnist
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# Load MNIST
(X_train, _), (_, _) = mnist.load_data()
X = X_train[:2000].reshape(-1, 28*28).astype(float)
print("MNIST subset shape:", X.shape)

# Scale
X_scaled = StandardScaler().fit_transform(X)

# PCA with 50 components
n_components = 50
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# Reconstruct
X_recon = pca.inverse_transform(X_pca).reshape(-1, 28, 28)

# Show first 8 original vs reconstructed
plt.figure(figsize=(12,4))
for i in range(8):
    # original
    plt.subplot(2, 8, i+1)
    plt.imshow(X[i].reshape(28,28), cmap="gray")
    plt.axis("off")
    # reconstructed
    plt.subplot(2, 8, i+9)
    plt.imshow(X_recon[i], cmap="gray")
    plt.axis("off")
plt.suptitle("MNIST Original (Top) vs Reconstructed (Bottom)")
plt.show()

# Explained variance plot
pca_full = PCA().fit(X_scaled)
cum_var = np.cumsum(pca_full.explained_variance_ratio_)
plt.plot(cum_var)
plt.xlabel("Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA - MNIST Explained Variance")
plt.grid(True)
plt.show()
print("Variance with 50 PCs:", cum_var[49])


================================================================================
LAB 3: ANN FROM SCRATCH - NEURAL NETWORK
================================================================================
Keywords: Lab_3, ANN_From_Scratch, Neural_Network, Backpropagation
Dataset: Iris Dataset
Implementation: Forward propagation, backpropagation, gradient descent
--------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# One-hot encode
Y = np.eye(3)[y]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Network architecture
input_size = X.shape[1]
hidden_size = 10
output_size = 3
learning_rate = 0.01

# Initialize weights
np.random.seed(42)
w1 = np.random.randn(input_size, hidden_size) * 0.01
b1 = np.zeros((1, hidden_size))
w2 = np.random.randn(hidden_size, output_size) * 0.01
b2 = np.zeros((1, output_size))

# Activation functions
def relu(z):
    return np.maximum(0, z)

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

def relu_backward(dA, z):
    dZ = np.array(dA, copy=True)
    dZ[z <= 0] = 0
    return dZ

# Training loop
epochs = 1000
losses = []

for epoch in range(epochs):
    # Forward prop
    Z1 = np.dot(X_train, w1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, w2) + b2
    A2 = softmax(Z2)

    # Compute Loss (Cross Entropy)
    m = y_train.shape[0]
    cost = -np.sum(y_train * np.log(A2 + 1e-8)) / m
    losses.append(cost)

    # Backward prop
    dZ2 = A2 - y_train
    dw2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    dA1 = np.dot(dZ2, w2.T)
    dZ1 = relu_backward(dA1, Z1)
    dw1 = np.dot(X_train.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    # Update weights
    w1 -= learning_rate * dw1
    b1 -= learning_rate * db1
    w2 -= learning_rate * dw2
    b2 -= learning_rate * db2

# Plot Loss Curve
plt.plot(losses)
plt.title("Loss Curve")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

# Prediction
Z1_test = np.dot(X_test, w1) + b1
A1_test = relu(Z1_test)
Z2_test = np.dot(A1_test, w2) + b2
A2_test = softmax(Z2_test)

predictions = np.argmax(A2_test, axis=1)
actuals = np.argmax(y_test, axis=1)
print("Accuracy:", accuracy_score(actuals, predictions))


================================================================================
LAB 3 (BONUS): DEEP ANN - TENSORFLOW/KERAS
================================================================================
Keywords: Lab_3, Deep_ANN, MNIST, TensorFlow
Dataset: MNIST
Features: Dropout, penultimate layer visualization
--------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Input
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA

# Load & preprocess MNIST
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784) / 255.0
x_test = x_test.reshape(-1, 784) / 255.0

# Build deep ANN
inputs = Input(shape=(784,))
x = Dense(256, activation='relu')(inputs)
x = Dropout(0.2)(x)
pen = Dense(64, activation='relu', name='pen_layer')(x)  # penultimate layer
outputs = Dense(10, activation='softmax')(pen)

model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(x_train, y_train, epochs=12, batch_size=256, validation_split=0.1, verbose=1)

# Plot Accuracy Curve
plt.figure(figsize=(6,4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title("Accuracy Curve")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# Plot Loss Curve
plt.figure(figsize=(6,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss Curve")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Confusion Matrix
y_pred = np.argmax(model.predict(x_test), axis=1)
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - MNIST")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Embedding of Penultimate Layer
pen_model = Model(model.input, model.get_layer('pen_layer').output)
pen_output = pen_model.predict(x_test)
pca = PCA(n_components=2)
embed = pca.fit_transform(pen_output)

plt.figure(figsize=(7,5))
for digit in range(10):
    idx = (y_test == digit)
    plt.scatter(embed[idx, 0], embed[idx, 1], s=5, label=str(digit), alpha=0.6)
plt.legend(title="Digit")
plt.title("2D PCA of Penultimate Layer Activations")
plt.show()


================================================================================
LAB 4: ACTIVATION FUNCTIONS
================================================================================
Keywords: Lab_4, Activation_Functions, Sigmoid, ReLU, Tanh
Functions: Sigmoid, Tanh, ReLU and their derivatives
Visualization: Function plots, gradient plots, heatmap comparison
--------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris

# Define activation functions
def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def sigmoid_grad(x):
    s = sigmoid(x)
    return s * (1 - s)

def tanh(x):
    return np.tanh(x)

def tanh_grad(x):
    return 1 - np.tanh(x)**2

def relu(x):
    return np.maximum(0, x)

def relu_grad(x):
    return (x > 0).astype(float)

# Generate input values
x = np.linspace(-6, 6, 200)

# Compute values
sig_vals = sigmoid(x)
sig_g = sigmoid_grad(x)
tanh_vals = tanh(x)
tanh_g = tanh_grad(x)
relu_vals = relu(x)
relu_g = relu_grad(x)

# Plot functions and gradients
plt.figure(figsize=(14, 8))

plt.subplot(3, 2, 1)
plt.plot(x, sig_vals)
plt.title("Sigmoid")
plt.grid(True)

plt.subplot(3, 2, 2)
plt.plot(x, sig_g)
plt.title("Sigmoid Derivative")
plt.grid(True)

plt.subplot(3, 2, 3)
plt.plot(x, tanh_vals, color="green")
plt.title("Tanh")
plt.grid(True)

plt.subplot(3, 2, 4)
plt.plot(x, tanh_g, color="green")
plt.title("Tanh Derivative")
plt.grid(True)

plt.subplot(3, 2, 5)
plt.plot(x, relu_vals, color="red")
plt.title("ReLU")
plt.grid(True)

plt.subplot(3, 2, 6)
plt.plot(x, relu_g, color="red")
plt.title("ReLU Derivative")
plt.grid(True)

plt.tight_layout()
plt.show()

# Heatmap comparison on Iris subset
iris = load_iris()
X_iris = iris.data[:5]  # first 5 samples

# Create a small random linear projection
np.random.seed(42)
W = np.random.randn(4, 3) * 0.5
b = np.zeros((1, 3))
Z = X_iris @ W + b

# Apply activation functions
A_sig = sigmoid(Z)
A_tanh = tanh(Z)
A_relu = relu(Z)

# Heatmap comparison
fig, axes = plt.subplots(1, 3, figsize=(12, 3))

sns.heatmap(A_sig, annot=True, cmap="Blues", ax=axes[0], cbar=False)
axes[0].set_title("Sigmoid activations")

sns.heatmap(A_tanh, annot=True, cmap="RdYlGn", ax=axes[1], cbar=False)
axes[1].set_title("Tanh activations")

sns.heatmap(A_relu, annot=True, cmap="OrRd", ax=axes[2], cbar=False)
axes[2].set_title("ReLU activations")

for ax in axes:
    ax.set_xlabel("Hidden unit")
    ax.set_ylabel("Sample index")

plt.tight_layout()
plt.show()


================================================================================
LAB 5 & 6: VACUUM CLEANER AGENT - RATIONAL AGENT
================================================================================
Keywords: Lab_5, Lab_6, Problem_Solving_Agent, Vacuum_Cleaner, Rational_Agent
Environment: 2x2 grid with random dirt
Features: Perception, action, performance tracking, dirt regeneration
--------------------------------------------------------------------------------

import random
import time
import matplotlib.pyplot as plt

# Environment Class
class Environment:
    def __init__(self):
        # Initialize a 2x2 grid environment with random "Clean" or "Dirty"
        self.rooms = {
            "A": random.choice(["Clean", "Dirty"]),
            "B": random.choice(["Clean", "Dirty"]),
            "C": random.choice(["Clean", "Dirty"]),
            "D": random.choice(["Clean", "Dirty"])
        }
        # Start agent at random location
        self.agent_location = random.choice(list(self.rooms.keys()))
        # Initialize performance score
        self.score = 0

    def display(self):
        print("\nEnvironment State:")
        print(f"A: {self.rooms['A']} | B: {self.rooms['B']}")
        print(f"C: {self.rooms['C']} | D: {self.rooms['D']}")
        print(f"Agent is at: {self.agent_location}")
        print(f"Current Score: {self.score}")
        print("-" * 30)

    def regenerate_dirt(self, probability=0.2):
        # Randomly make some rooms dirty again
        for room in self.rooms:
            if self.rooms[room] == "Clean" and random.random() < probability:
                self.rooms[room] = "Dirty"
                print(f"Room {room} became dirty again!")

# Vacuum Cleaner Agent Class
class VacuumAgent:
    def __init__(self, environment):
        self.env = environment
        self.performance_log = []  # For plotting

    def perceive_and_act(self):
        location = self.env.agent_location
        status = self.env.rooms[location]

        if status == "Dirty":
            print(f"Cleaning room {location}...")
            self.env.rooms[location] = "Clean"
            self.env.score += 10
        else:
            # Move to another room
            next_room = random.choice(list(self.env.rooms.keys()))
            while next_room == location:
                next_room = random.choice(list(self.env.rooms.keys()))
            print(f"Moving from {location} -> {next_room}")
            self.env.agent_location = next_room
            self.env.score -= 1

        # Log performance
        self.performance_log.append(self.env.score)

# Simulation Function
def run_simulation(steps=15):
    env = Environment()
    agent = VacuumAgent(env)
    print("Initial Environment:")
    env.display()

    for step in range(steps):
        print(f"\nStep {step + 1}/{steps}")
        agent.perceive_and_act()
        env.regenerate_dirt()
        env.display()
        time.sleep(0.3)

    print("\nSimulation Complete!")
    print(f"Final Room States: {env.rooms}")
    print(f"Final Score: {env.score}")
    
    # Plot performance
    plt.figure(figsize=(8, 4))
    plt.plot(agent.performance_log, marker='o')
    plt.title("Performance Over Time")
    plt.xlabel("Step Number")
    plt.ylabel("Performance Score")
    plt.grid(True)
    plt.show()

# Run Simulation
run_simulation(steps=20)


================================================================================
LAB 7: GENETIC ALGORITHM - OPTIMIZATION
================================================================================
Keywords: Lab_7, Genetic_Algorithm, Optimization, Non_Gradient
Problem: Minimize Rastrigin function
Components: Selection, crossover, mutation, fitness evaluation
Visualization: Heatmap, population evolution, convergence plot
--------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt

# Define Rastrigin function
def rastrigin(X):
    X, Y = X
    return 20 + (X**2 - 10 * np.cos(2 * np.pi * X)) + (Y**2 - 10 * np.cos(2 * np.pi * Y))

# Initialize Population
def initialize_population(pop_size, bounds):
    population = np.random.uniform(low=bounds[0], high=bounds[1], size=(pop_size, 2))
    return population

# Fitness Function
def fitness(population):
    return 1 / (1 + np.array([rastrigin(ind) for ind in population]))

# Selection (Roulette Wheel)
def select_parents(population, fitness_vals):
    probs = fitness_vals / np.sum(fitness_vals)
    selected_idx = np.random.choice(len(population), size=2, replace=False, p=probs)
    return population[selected_idx]

# Crossover (Blending)
def crossover(parent1, parent2):
    alpha = np.random.rand()
    child1 = alpha * parent1 + (1 - alpha) * parent2
    child2 = alpha * parent2 + (1 - alpha) * parent1
    return child1, child2

# Mutation
def mutate(child, mutation_rate=0.1):
    if np.random.rand() < mutation_rate:
        mutation = np.random.uniform(-0.5, 0.5, size=2)
        child += mutation
    return child

# Plot Heatmap Function
def plot_heatmap(bounds, population=None, best=None, generation=None):
    x = np.linspace(bounds[0], bounds[1], 200)
    y = np.linspace(bounds[0], bounds[1], 200)
    X, Y = np.meshgrid(x, y)
    Z = 20 + X**2 + Y**2 - 10 * (np.cos(2 * np.pi * X) + np.cos(2 * np.pi * Y))
    
    plt.figure(figsize=(7, 6))
    plt.contourf(X, Y, Z, levels=50, cmap="plasma")
    plt.colorbar(label='Rastrigin Function Value')
    
    if population is not None:
        plt.scatter(population[:, 0], population[:, 1], color="white", edgecolors="black", label="Population")
    
    if best is not None:
        plt.scatter(best[0], best[1], color='red', s=80, marker='*', label='Best Solution')

    plt.title(f"GA Population - Generation {generation}")
    plt.xlabel("X-axis")
    plt.ylabel("Y-axis")
    plt.legend()
    plt.show()

# Main GA Function
def genetic_algorithm(generations=50, pop_size=30, bounds=[-5.12, 5.12]):
    population = initialize_population(pop_size, bounds)
    best_scores = []

    print("Starting Genetic Algorithm...\n")

    for gen in range(generations):
        fitness_vals = fitness(population)
        new_population = []
        
        # Keep best individual (elitism)
        best_idx = np.argmax(fitness_vals)
        best_individual = population[best_idx]
        best_scores.append(rastrigin(best_individual))
        
        # Print progress
        if gen % 10 == 0 or gen == generations - 1:
            print(f"Generation {gen}: Best Fitness Value = {rastrigin(best_individual):.4f}")
            plot_heatmap(bounds, population, best_individual, generation=gen)
        
        # Create new generation
        while len(new_population) < pop_size:
            parent1, parent2 = select_parents(population, fitness_vals)
            child1, child2 = crossover(parent1, parent2)
            mutate(child1)
            mutate(child2)
            new_population.extend([child1, child2])

        population = np.array(new_population[:pop_size])

    # Final best individual
    best_idx = np.argmax(fitness(population))
    best_solution = population[best_idx]

    print("\nBest Solution Found:")
    print(f"x = {best_solution[0]:.4f}, y = {best_solution[1]:.4f}")
    print(f"Rastrigin Value = {rastrigin(best_solution):.4f}")

    # Plot convergence
    plt.figure(figsize=(7,5))
    plt.plot(best_scores, color='purple', linewidth=2)
    plt.title("Genetic Algorithm Convergence")
    plt.xlabel("Generations")
    plt.ylabel("Best Rastrigin Value")
    plt.grid(True)
    plt.show()

# Run GA
genetic_algorithm()


================================================================================
LAB 8: CONSTRAINT OPTIMIZATION - LINEAR PROGRAMMING
================================================================================
Keywords: Lab_8, Constraint_Optimization, Resource_Allocation, Linear_Programming
Problem: Maximize profit with labor and material constraints
Method: scipy.optimize.linprog
Visualization: 2D feasible region, 3D profit surface
--------------------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import linprog
from mpl_toolkits.mplot3d import Axes3D

# Define coefficients
# Objective: Maximize Z = 30x1 + 20x2 (linprog minimizes, so use negative)
c = [-30, -20]

# Define constraints
# 2x1 + 3x2 <= 100 (Labor)
# 3x1 + 2x2 <= 90 (Material)
A = [[2, 3], [3, 2]]
b = [100, 90]

# Define variable bounds
x_bounds = (0, None)
bounds = [x_bounds, x_bounds]

# Solve optimization problem
result = linprog(c, A_ub=A, b_ub=b, bounds=bounds, method="highs")

if result.success:
    x1, x2 = result.x
    print("Optimization Successful!")
    print(f"Optimal Production of Product A: {x1:.2f} units")
    print(f"Optimal Production of Product B: {x2:.2f} units")
    print(f"Maximum Profit: ₹{-result.fun:.2f}")
else:
    print("Optimization Failed:", result.message)

# 2D Feasible Region Visualization
x = np.linspace(0, 50, 200)
y1 = (100 - 2*x) / 3  # Labor constraint
y2 = (90 - 3*x) / 2   # Material constraint

# Feasible region
y3 = np.minimum(y1, y2)
y3[y3 < 0] = np.nan

plt.figure(figsize=(8,6))
plt.fill_between(x, y3, color='lightgreen', alpha=0.4, label="Feasible Region")
plt.plot(x, y1, color='red', label=r'$2x_1 + 3x_2 \leq 100$')
plt.plot(x, y2, color='blue', label=r'$3x_1 + 2x_2 \leq 90$')
plt.scatter(x1, x2, color='black', s=200, marker='*', label='Optimal Point')
plt.xlabel("Product A (x1)")
plt.ylabel("Product B (x2)")
plt.title("2D Feasible Region for Resource Allocation")
plt.legend()
plt.grid(True)
plt.show()

# 3D Profit Surface Visualization
X, Y = np.meshgrid(np.linspace(0, 50, 100), np.linspace(0, 50, 100))
Z = 30*X + 20*Y  # Profit function

# Apply constraints (make infeasible points NaN)
Z[(2*X + 3*Y > 100) | (3*X + 2*Y > 90)] = np.nan

fig = plt.figure(figsize=(10,7))
ax = fig.add_subplot(111, projection='3d')

# Surface plot
surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.9, edgecolor='none')
fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label="Profit (₹)")

# Mark constraints lines (projected approx)
ax.plot(x, y1, np.zeros_like(x), color='red', linewidth=2, label='Labor Constraint')
ax.plot(x, y2, np.zeros_like(x), color='blue', linewidth=2, label='Material Constraint')

# Mark optimal point in 3D
ax.scatter(x1, x2, 30*x1 + 20*x2, color='black', s=100, marker='*', label='Optimal Solution')

# Labels
ax.set_xlabel("Product A (x1)")
ax.set_ylabel("Product B (x2)")
ax.set_zlabel("Profit (₹)")
ax.set_title("3D Profit Surface with Constraints")
ax.legend()
plt.show()


================================================================================
END OF ALL LABS
================================================================================

Lab Summary:
1. Linear Regression & Classification (Supervised Learning)
2. K-Means Clustering & PCA (Unsupervised Learning)
3. ANN from Scratch & Deep ANN (Neural Networks)
4. Activation Functions (Sigmoid, ReLU, Tanh)
5. Vacuum Cleaner Agent (Problem Solving Agent)
6. Rational Agent Simulation (included in Lab 5)
7. Genetic Algorithm (Non-gradient Optimization)
8. Linear Programming (Constraint-based Optimization)

================================================================================
